"""
ðŸŽ¯ PARSER ENGINE - Advanced HTML Parsing & Data Extraction for CoffeCrawler
Revolutionary parsing with AI-powered extraction, multi-backend support, and smart data processing.
My data is to much.
"""

import re
import json
from typing import Dict, List, Any, Optional, Union, Tuple
from urllib.parse import urljoin, urlparse
from dataclasses import dataclass
import html as html_lib
from bs4 import BeautifulSoup, FeatureNotFound
import lxml.html
import cssselect
from functools import lru_cache

from ..exceptions import ParserError, DataExtractionError
from ..utils.performance_optimizer import PerformanceOptimizer


@dataclass
class ExtractionResult:
    """Structured extraction result container"""
    success: bool
    data: Any
    extraction_method: str
    confidence: float
    metadata: Dict[str, Any]
    errors: List[str]


class ParserEngine:
    """
    ðŸŽ¯ ADVANCED PARSER ENGINE - Revolutionary HTML Parsing & Data Extraction
    
    Features:
    - Multi-backend support (BeautifulSoup, lxml, parsel, native)
    - AI-powered intelligent extraction
    - Smart pattern recognition
    - Automatic data normalization
    - XPath and CSS selector support
    - JSON-LD and microdata extraction
    - Error-resistant parsing
    - Performance optimization
    """
    
    def __init__(self, crawler):
        self.crawler = crawler
        self.available_backends = self._detect_available_backends()
        self.primary_backend = self._select_primary_backend()
        self.performance_monitor = ParserPerformanceMonitor()
        
        # Advanced features
        self.enable_ai_extraction = True
        self.smart_fallback = True
        self.auto_normalization = True
        self.extraction_cache = {}
        
        # Initialize backends
        self._initialize_backends()
        
        if crawler.debug_mode:
            print(f"ðŸŽ¯ Parser Engine initialized - Backend: {self.primary_backend.upper()}")
            print(f"   Available backends: {list(self.available_backends.keys())}")
    
    def _detect_available_backends(self) -> Dict[str, bool]:
        """Detect available parsing backends"""
        backends = {}
        
        try:
            import bs4
            backends['beautifulsoup'] = True
        except ImportError:
            backends['beautifulsoup'] = False
        
        try:
            import lxml.html
            backends['lxml'] = True
        except ImportError:
            backends['lxml'] = False
        
        try:
            import parsel
            backends['parsel'] = True
        except ImportError:
            backends['parsel'] = False
        
        # Native backend is always available
        backends['native'] = True
        
        return backends
    
    def _select_primary_backend(self) -> str:
        """Select the primary parsing backend"""
        # Priority: lxml > beautifulsoup > parsel > native
        if self.available_backends.get('lxml'):
            return 'lxml'
        elif self.available_backends.get('beautifulsoup'):
            return 'beautifulsoup'
        elif self.available_backends.get('parsel'):
            return 'parsel'
        else:
            return 'native'
    
    def _initialize_backends(self):
        """Initialize parsing backends"""
        if self.available_backends['beautifulsoup']:
            self.bs_parser = self._get_bs_parser()
        
        if self.available_backends['lxml']:
            self.lxml_parser = lxml.html.HTMLParser(encoding='utf-8', recover=True)
    
    def _get_bs_parser(self) -> str:
        """Get the best BeautifulSoup parser"""
        parsers = ['lxml', 'html.parser', 'html5lib']
        
        for parser in parsers:
            try:
                # Test the parser
                BeautifulSoup("<html></html>", parser)
                return parser
            except FeatureNotFound:
                continue
        
        return 'html.parser'  # Fallback
    
    def extract(self, 
                raw_data: Any, 
                extract_rules: Union[List[str], Dict[str, Any], str],
                strategy: Dict) -> ExtractionResult:
        """
        ðŸŽ¯ MAIN EXTRACTION METHOD - Advanced Data Extraction with Multiple Strategies
        
        Args:
            raw_data: Raw HTML content or response object
            extract_rules: Rules for data extraction
            strategy: Extraction strategy
        
        Returns:
            ExtractionResult: Structured extraction result
        """
        start_time = time.time()
        
        try:
            # Extract HTML content from raw data
            html_content = self._extract_html_content(raw_data)
            
            if self.crawler.debug_mode:
                print(f"   ðŸ“„ Parsing HTML ({len(html_content)} chars)")
            
            # Parse HTML based on backend
            parsed_doc = self._parse_html(html_content)
            
            # Process extraction rules
            if isinstance(extract_rules, str):
                # Single rule
                result = self._extract_single_rule(parsed_doc, extract_rules, strategy)
            elif isinstance(extract_rules, list):
                # Multiple rules
                result = self._extract_multiple_rules(parsed_doc, extract_rules, strategy)
            elif isinstance(extract_rules, dict):
                # Structured rules
                result = self._extract_structured_rules(parsed_doc, extract_rules, strategy)
            else:
                raise ParserError("Invalid extract_rules format")
            
            # Apply post-processing
            if self.auto_normalization:
                result = self._normalize_extraction_result(result, strategy)
            
            # Performance monitoring
            processing_time = time.time() - start_time
            self.performance_monitor.record_extraction(processing_time, True)
            
            return ExtractionResult(
                success=True,
                data=result,
                extraction_method=self.primary_backend,
                confidence=1.0,
                metadata={
                    'processing_time': processing_time,
                    'content_length': len(html_content),
                    'backend_used': self.primary_backend
                },
                errors=[]
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.performance_monitor.record_extraction(processing_time, False)
            
            if self.crawler.debug_mode:
                print(f"   âŒ Extraction failed: {e}")
            
            # Fallback extraction attempt
            if self.smart_fallback:
                fallback_result = self._fallback_extraction(raw_data, extract_rules)
                if fallback_result:
                    return fallback_result
            
            return ExtractionResult(
                success=False,
                data=None,
                extraction_method='error',
                confidence=0.0,
                metadata={'processing_time': processing_time},
                errors=[str(e)]
            )
    
    def _extract_html_content(self, raw_data: Any) -> str:
        """Extract HTML content from various input types"""
        if hasattr(raw_data, 'content'):
            # Response object with content
            if isinstance(raw_data.content, str):
                return raw_data.content
            else:
                return raw_data.content.decode('utf-8', errors='ignore')
        elif hasattr(raw_data, 'text'):
            # Response object with text
            return raw_data.text
        elif isinstance(raw_data, dict) and 'content' in raw_data:
            # Dictionary with content
            return raw_data['content']
        else:
            # Assume it's already HTML string
            return str(raw_data)
    
    def _parse_html(self, html_content: str) -> Any:
        """Parse HTML content using the primary backend"""
        if self.primary_backend == 'beautifulsoup':
            return BeautifulSoup(html_content, self.bs_parser)
        elif self.primary_backend == 'lxml':
            return lxml.html.fromstring(html_content)
        elif self.primary_backend == 'parsel':
            import parsel
            return parsel.Selector(html_content)
        else:
            # Native parsing (basic)
            return html_content
    
    def _extract_single_rule(self, parsed_doc, rule: str, strategy: Dict) -> Any:
        """Extract data using a single rule"""
        extraction_methods = [
            self._extract_css_selector,
            self._extract_xpath,
            self._extract_regex,
            self._extract_smart_pattern
        ]
        
        for method in extraction_methods:
            try:
                result = method(parsed_doc, rule, strategy)
                if result and self._validate_extraction_result(result):
                    return result
            except Exception:
                continue
        
        # If all methods fail, try AI-powered extraction
        if self.enable_ai_extraction:
            return self._ai_extract_single(parsed_doc, rule, strategy)
        
        raise DataExtractionError(f"No extraction method worked for rule: {rule}")
    
    def _extract_multiple_rules(self, parsed_doc, rules: List[str], strategy: Dict) -> Dict[str, Any]:
        """Extract data using multiple rules"""
        results = {}
        
        for rule in rules:
            try:
                results[rule] = self._extract_single_rule(parsed_doc, rule, strategy)
            except Exception as e:
                if self.crawler.debug_mode:
                    print(f"   âš ï¸ Rule '{rule}' failed: {e}")
                results[rule] = None
        
        return results
    
    def _extract_structured_rules(self, parsed_doc, rules: Dict[str, Any], strategy: Dict) -> Dict[str, Any]:
        """Extract data using structured rules"""
        results = {}
        
        for field_name, rule_config in rules.items():
            try:
                if isinstance(rule_config, str):
                    # Simple selector
                    results[field_name] = self._extract_single_rule(parsed_doc, rule_config, strategy)
                elif isinstance(rule_config, dict):
                    # Advanced configuration
                    results[field_name] = self._extract_advanced_rule(parsed_doc, rule_config, strategy)
                else:
                    results[field_name] = None
                    
            except Exception as e:
                if self.crawler.debug_mode:
                    print(f"   âš ï¸ Field '{field_name}' extraction failed: {e}")
                results[field_name] = None
        
        return results
    
    def _extract_advanced_rule(self, parsed_doc, rule_config: Dict, strategy: Dict) -> Any:
        """Extract data using advanced rule configuration"""
        # Multiple selector options
        selectors = rule_config.get('selectors', [])
        extraction_type = rule_config.get('type', 'text')
        multiple = rule_config.get('multiple', False)
        default = rule_config.get('default', None)
        
        for selector in selectors:
            try:
                result = self._extract_single_rule(parsed_doc, selector, strategy)
                
                if result:
                    # Apply type conversion
                    if extraction_type == 'number':
                        result = self._convert_to_number(result)
                    elif extraction_type == 'boolean':
                        result = self._convert_to_boolean(result)
                    elif extraction_type == 'array':
                        result = self._convert_to_array(result)
                    
                    return result
                    
            except Exception:
                continue
        
        return default
    
    def _extract_css_selector(self, parsed_doc, selector: str, strategy: Dict) -> Any:
        """Extract using CSS selector"""
        if self.primary_backend == 'beautifulsoup':
            elements = parsed_doc.select(selector)
            return [elem.get_text(strip=True) for elem in elements] if elements else None
        
        elif self.primary_backend == 'lxml':
            elements = parsed_doc.cssselect(selector)
            return [elem.text_content().strip() for elem in elements] if elements else None
        
        elif self.primary_backend == 'parsel':
            elements = parsed_doc.css(selector)
            return elements.getall() if elements else None
        
        else:
            # Native CSS selector (basic)
            return self._native_css_extract(parsed_doc, selector)
    
    def _extract_xpath(self, parsed_doc, xpath: str, strategy: Dict) -> Any:
        """Extract using XPath"""
        if self.primary_backend == 'beautifulsoup':
            # BeautifulSoup doesn't support XPath directly
            return None
        
        elif self.primary_backend == 'lxml':
            elements = parsed_doc.xpath(xpath)
            if isinstance(elements, list):
                return [elem.text_content().strip() if hasattr(elem, 'text_content') else str(elem) 
                       for elem in elements]
            else:
                return elements.text_content().strip() if hasattr(elements, 'text_content') else str(elements)
        
        elif self.primary_backend == 'parsel':
            elements = parsed_doc.xpath(xpath)
            return elements.getall() if elements else None
        
        else:
            return None
    
    def _extract_regex(self, parsed_doc, pattern: str, strategy: Dict) -> Any:
        """Extract using regular expressions"""
        if self.primary_backend in ['beautifulsoup', 'lxml', 'parsel']:
            html_content = self._get_html_content(parsed_doc)
        else:
            html_content = parsed_doc
        
        matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
        return matches if matches else None
    
    def _extract_smart_pattern(self, parsed_doc, pattern: str, strategy: Dict) -> Any:
        """Extract using smart pattern recognition"""
        # Common patterns
        patterns = {
            'emails': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'phones': r'(\+?\d{1,3}[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}',
            'prices': r'[\$â‚¬Â£Â¥]?(\d+[.,]\d{2})',
            'urls': r'https?://[^\s<>"]+|www\.[^\s<>"]+',
            'dates': r'\b\d{1,2}[-/]\d{1,2}[-/]\d{2,4}\b'
        }
        
        if pattern in patterns:
            html_content = self._get_html_content(parsed_doc)
            matches = re.findall(patterns[pattern], html_content)
            return matches if matches else None
        
        return None
    
    def _ai_extract_single(self, parsed_doc, rule: str, strategy: Dict) -> Any:
        """AI-powered extraction (placeholder for advanced implementation)"""
        # This would integrate with ML models in a real implementation
        # For now, we use advanced heuristics
        
        html_content = self._get_html_content(parsed_doc)
        
        # Advanced heuristic extraction based on rule type
        if 'product' in rule.lower():
            return self._extract_products_heuristic(html_content)
        elif 'price' in rule.lower():
            return self._extract_prices_heuristic(html_content)
        elif 'title' in rule.lower():
            return self._extract_title_heuristic(html_content)
        else:
            return None
    
    def _extract_products_heuristic(self, html_content: str) -> List[str]:
        """Heuristic product name extraction"""
        # Look for common product patterns
        patterns = [
            r'<h[1-6][^>]*>(.*?)</h[1-6]>',
            r'class="[^"]*product[^"]*"[^>]*>.*?<h[1-6][^>]*>(.*?)</h[1-6]>',
            r'<div[^>]*class="[^"]*title[^"]*"[^>]*>(.*?)</div>'
        ]
        
        products = []
        for pattern in patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
            products.extend([match.strip() for match in matches if len(match.strip()) > 5])
        
        return list(set(products))[:10]  # Return unique products, limit to 10
    
    def _extract_prices_heuristic(self, html_content: str) -> List[str]:
        """Heuristic price extraction"""
        price_patterns = [
            r'[\$â‚¬Â£Â¥]?\s*(\d{1,3}(?:[.,]\d{3})*[.,]\d{2})',
            r'price["\']?\s*[:=]\s*["\']?([\$â‚¬Â£Â¥]?\s*\d+[.,]\d{2})',
            r'class="[^"]*price[^"]*"[^>]*>([^<]+)'
        ]
        
        prices = []
        for pattern in price_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            prices.extend(matches)
        
        return list(set(prices))
    
    def _extract_title_heuristic(self, html_content: str) -> str:
        """Heuristic title extraction"""
        # Try to find the main title
        title_patterns = [
            r'<title[^>]*>(.*?)</title>',
            r'<h1[^>]*>(.*?)</h1>',
            r'<meta[^>]*property="og:title"[^>]*content="([^"]*)"',
            r'<meta[^>]*name="twitter:title"[^>]*content="([^"]*)"'
        ]
        
        for pattern in title_patterns:
            match = re.search(pattern, html_content, re.IGNORECASE)
            if match:
                title = match.group(1).strip()
                if title and len(title) > 3:
                    return html_lib.unescape(title)
        
        return None
    
    def _get_html_content(self, parsed_doc) -> str:
        """Get HTML content from parsed document"""
        if self.primary_backend == 'beautifulsoup':
            return str(parsed_doc)
        elif self.primary_backend == 'lxml':
            return lxml.html.tostring(parsed_doc, encoding='unicode')
        elif self.primary_backend == 'parsel':
            return parsed_doc.get()
        else:
            return parsed_doc
    
    def _validate_extraction_result(self, result: Any) -> bool:
        """Validate extraction result"""
        if result is None:
            return False
        if isinstance(result, (list, tuple)) and len(result) == 0:
            return False
        if isinstance(result, str) and not result.strip():
            return False
        return True
    
    def _normalize_extraction_result(self, result: Any, strategy: Dict) -> Any:
        """Normalize extraction result"""
        if isinstance(result, list):
            return [self._normalize_value(item, strategy) for item in result]
        else:
            return self._normalize_value(result, strategy)
    
    def _normalize_value(self, value: Any, strategy: Dict) -> Any:
        """Normalize a single value"""
        if not isinstance(value, str):
            return value
        
        # Clean up whitespace
        value = value.strip()
        
        # Remove extra whitespace
        value = re.sub(r'\s+', ' ', value)
        
        # HTML unescape
        value = html_lib.unescape(value)
        
        return value
    
    def _convert_to_number(self, value: Any) -> Union[int, float, None]:
        """Convert value to number"""
        if isinstance(value, (int, float)):
            return value
        
        if isinstance(value, str):
            # Remove non-numeric characters except decimal point
            numeric_str = re.sub(r'[^\d.,]', '', value)
            if numeric_str:
                try:
                    # Handle different decimal separators
                    if ',' in numeric_str and '.' in numeric_str:
                        # European format: 1.000,00 -> 1000.00
                        numeric_str = numeric_str.replace('.', '').replace(',', '.')
                    elif ',' in numeric_str:
                        # European format: 1,00 -> 1.00
                        numeric_str = numeric_str.replace(',', '.')
                    
                    return float(numeric_str)
                except ValueError:
                    pass
        
        return None
    
    def _convert_to_boolean(self, value: Any) -> bool:
        """Convert value to boolean"""
        if isinstance(value, bool):
            return value
        
        if isinstance(value, (int, float)):
            return bool(value)
        
        if isinstance(value, str):
            true_values = ['true', 'yes', '1', 'on', 'enabled']
            false_values = ['false', 'no', '0', 'off', 'disabled']
            
            if value.lower() in true_values:
                return True
            elif value.lower() in false_values:
                return False
        
        return bool(value)
    
    def _convert_to_array(self, value: Any) -> List[Any]:
        """Convert value to array"""
        if isinstance(value, list):
            return value
        elif isinstance(value, tuple):
            return list(value)
        else:
            return [value]
    
    def _native_css_extract(self, html_content: str, selector: str) -> Any:
        """Basic CSS selector extraction for native backend"""
        # Very basic implementation - in production, use proper parser
        if selector.startswith('.'):
            # Class selector
            class_name = selector[1:]
            pattern = f'class="[^"]*{class_name}[^"]*"[^>]*>(.*?)</'
            matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
            return matches if matches else None
        
        elif selector.startswith('#'):
            # ID selector
            id_name = selector[1:]
            pattern = f'id="{id_name}"[^>]*>(.*?)</'
            match = re.search(pattern, html_content, re.IGNORECASE | re.DOTALL)
            return [match.group(1)] if match else None
        
        else:
            # Tag selector
            pattern = f'<{selector}[^>]*>(.*?)</{selector}>'
            matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
            return matches if matches else None
    
    def _fallback_extraction(self, raw_data: Any, extract_rules: Any) -> Optional[ExtractionResult]:
        """Fallback extraction when primary methods fail"""
        try:
            html_content = self._extract_html_content(raw_data)
            
            # Try simple text extraction
            if isinstance(extract_rules, str) and extract_rules in ['text', 'content']:
                # Extract all text content
                text_content = re.sub(r'<[^>]+>', ' ', html_content)
                text_content = re.sub(r'\s+', ' ', text_content).strip()
                
                return ExtractionResult(
                    success=True,
                    data=text_content,
                    extraction_method='fallback_text',
                    confidence=0.7,
                    metadata={'fallback_used': True},
                    errors=['Primary extraction failed, used fallback']
                )
            
        except Exception:
            pass
        
        return None
    
    def extract_metadata(self, raw_data: Any) -> Dict[str, Any]:
        """Extract metadata from HTML content"""
        html_content = self._extract_html_content(raw_data)
        
        metadata = {
            'title': self._extract_title_heuristic(html_content),
            'description': self._extract_meta_description(html_content),
            'keywords': self._extract_meta_keywords(html_content),
            'language': self._extract_language(html_content),
            'charset': self._extract_charset(html_content),
            'links': self._extract_links(html_content),
            'images': self._extract_images(html_content),
        }
        
        return {k: v for k, v in metadata.items() if v is not None}
    
    def _extract_meta_description(self, html_content: str) -> Optional[str]:
        """Extract meta description"""
        pattern = r'<meta[^>]*name="description"[^>]*content="([^"]*)"'
        match = re.search(pattern, html_content, re.IGNORECASE)
        return html_lib.unescape(match.group(1)) if match else None
    
    def _extract_meta_keywords(self, html_content: str) -> Optional[str]:
        """Extract meta keywords"""
        pattern = r'<meta[^>]*name="keywords"[^>]*content="([^"]*)"'
        match = re.search(pattern, html_content, re.IGNORECASE)
        return html_lib.unescape(match.group(1)) if match else None
    
    def _extract_language(self, html_content: str) -> Optional[str]:
        """Extract language"""
        pattern = r'<html[^>]*lang="([^"]*)"'
        match = re.search(pattern, html_content, re.IGNORECASE)
        return match.group(1) if match else None
    
    def _extract_charset(self, html_content: str) -> Optional[str]:
        """Extract charset"""
        pattern = r'<meta[^>]*charset="([^"]*)"'
        match = re.search(pattern, html_content, re.IGNORECASE)
        return match.group(1) if match else None
    
    def _extract_links(self, html_content: str) -> List[str]:
        """Extract all links"""
        pattern = r'<a[^>]*href="([^"]*)"'
        matches = re.findall(pattern, html_content, re.IGNORECASE)
        return [html_lib.unescape(link) for link in matches if link]
    
    def _extract_images(self, html_content: str) -> List[str]:
        """Extract all image sources"""
        pattern = r'<img[^>]*src="([^"]*)"'
        matches = re.findall(pattern, html_content, re.IGNORECASE)
        return [html_lib.unescape(src) for src in matches if src]
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get parser performance statistics"""
        return self.performance_monitor.get_stats()


class ParserPerformanceMonitor:
    """Advanced parser performance monitoring"""
    
    def __init__(self):
        self.metrics = {
            'total_extractions': 0,
            'successful_extractions': 0,
            'failed_extractions': 0,
            'average_processing_time': 0,
            'total_processing_time': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
    
    def record_extraction(self, processing_time: float, success: bool):
        """Record extraction performance"""
        self.metrics['total_extractions'] += 1
        self.metrics['total_processing_time'] += processing_time
        
        if success:
            self.metrics['successful_extractions'] += 1
        else:
            self.metrics['failed_extractions'] += 1
        
        # Update average
        self.metrics['average_processing_time'] = (
            self.metrics['total_processing_time'] / self.metrics['total_extractions']
        )
    
    def record_cache_hit(self, hit: bool):
        """Record cache performance"""
        if hit:
            self.metrics['cache_hits'] += 1
        else:
            self.metrics['cache_misses'] += 1
    
    def get_stats(self) -> Dict[str, Any]:
        """Get performance statistics"""
        stats = self.metrics.copy()
        
        # Calculate success rate
        if stats['total_extractions'] > 0:
            stats['success_rate'] = (
                stats['successful_extractions'] / stats['total_extractions'] * 100
            )
        else:
            stats['success_rate'] = 0
        
        # Calculate cache hit rate
        total_cache_attempts = stats['cache_hits'] + stats['cache_misses']
        if total_cache_attempts > 0:
            stats['cache_hit_rate'] = (
                stats['cache_hits'] / total_cache_attempts * 100
            )
        else:
            stats['cache_hit_rate'] = 0
        
        return stats


# Factory function for easy creation
def create_parser_engine(crawler, backend: str = 'auto'):
    """Factory function to create parser engine instance"""
    parser = ParserEngine(crawler)
    
    # Override backend if specified
    if backend != 'auto' and backend in parser.available_backends:
        parser.primary_backend = backend
    
    return parser


print("âœ… Parser Engine loaded successfully!")
